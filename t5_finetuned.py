# -*- coding: utf-8 -*-
"""t5-finetuned (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dAiMRH7Wsv_aH0C-J1diiQCQJEiMbMRS

# Fine Tuning T5-base Model for Pubmed Dataset

##1. Install Transformers and Datasets from Hugging Face
"""

# Install transformer
! pip install -q transformers[torch] datasets

"""##2. Load Dataset from Hugging Face"""

# Import data
from datasets import load_dataset

dataset = load_dataset('ccdv/pubmed-summarization')

# Examine the structure of dataset
dataset

# Subset the training, validation and test sets data
# Randomly pick 1000 rows for training, and 125 rows for both validation and testing
dataset["train"] = dataset["train"].shuffle(seed=42).select(range(1000))
dataset["validation"] = dataset["validation"].shuffle(seed=42).select(range(125))
dataset["test"] = dataset["test"].shuffle(seed=42).select(range(125))

# Check the structure of dataset
dataset

"""##3. Preprocessing"""

# Define the tokenizer: t5-base as tokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-base")

def preprocess_function(examples):
    # Attach the prefix "summarize: " to instruct the T5 model on the task it needs to perform
    inputs = ["summarize: " + doc for doc in examples['article']]

    # Tokenise the input texts
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

    # Tokenise the 'abstract' field of the inputs to prepare target labels
    labels = tokenizer(text_target=examples["abstract"], max_length=256, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply preprocessing to the dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True)

"""##4. Create Batches Using Data Collator"""

# Create a batch of examples
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model="t5-base")

"""##5. Define Evaluation Metrics for Training"""

! pip install -q evaluate rouge_score

import evaluate

rouge = evaluate.load("rouge")

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    # Decode the tokenised predictions into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # Decodes the tokenised labels into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Computes the ROUGE scores between decoded predictions and decoded target
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)
    return {k: round(v, 4) for k, v in result.items()}

"""##6. Train"""

from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Load the T5-small model
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

# Define training parameters
training_args = Seq2SeqTrainingArguments(
    output_dir="fine_tuned_t5_small_model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16 = True,
)

# Pass the arguments to Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Finetune the model
trainer.train()

"""##7. Save the Model"""

trainer.save_model("fine_tuned_t5_small_model")

"""##8. Use the Fine-Tuned Model to Summarize Text"""

# Split the test data to 1. article to summarise (texts) 2. reference summary (target)
texts = dataset['test']['article']
target = dataset['test']['abstract']

type(target)

# Import tokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("fine_tuned_t5_small_model")

# Import model
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("fine_tuned_t5_small_model")

import pandas as pd
# Initialise an empty list to store the summaries
summaries = []

# Loop through each text in the column
for text in texts:
    # Tokenise the text
    tokens_input = tokenizer.encode("summarize: " + text, return_tensors='pt', max_length=512, truncation=True)

    # Generate summary
    summary_ids = model.generate(tokens_input, min_length=30, max_length=512)

    # Decode the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    # Append the summary to the list of summaries
    summaries.append(summary)

# Store results
results = {
    'article': texts,
    'abstract': target,
    'model summary': summaries
}
results = pd.DataFrame(results, columns = ['article', 'abstract', 'model summary'])

results.to_csv('pubmed_summary.csv', index = False)

results.to_excel('pubmed_summary.xlsx', index = False)

results.head()

"""##9. Use Metrics to Evaluate Results"""

predictions = list(results['model summary'])
references = target

# Compute ROUGE score
from datasets import load_metric
rouge = load_metric("rouge")
results_rouge = rouge.compute(predictions = predictions, references = references)

from typing import Dict, Any

def simplify_rouge_scores(rouge_scores: Dict[str, Any]) -> str:
    simplified_text = ""
    mean_rouge = {}
    for key, value in rouge_scores.items():
        # Extract low, mid, and high scores for each ROUGE metric
        low, mid, high = value.low, value.mid, value.high
        simplified_text += f"{key}: Precision ranges from {low.precision:.2%} to {high.precision:.2%}, "
        simplified_text += f"Recall ranges from {low.recall:.2%} to {high.recall:.2%}, "
        simplified_text += f"F1 Score ranges from {low.fmeasure:.2%} to {high.fmeasure:.2%}.\n"
        mean_rouge[f"{key}"] = [round(mid.precision,4), round(mid.recall,4), round(mid.fmeasure,4)]

    return simplified_text, mean_rouge

text, rouge_scores = simplify_rouge_scores(results_rouge)

# Show ROUGE scores (medians)
rouge_scores = pd.DataFrame(rouge_scores)
rouge_scores.index = ['Precision', "Recall", "F-Measure"]
print(rouge_scores)

print(simplify_rouge_scores(results_rouge))
