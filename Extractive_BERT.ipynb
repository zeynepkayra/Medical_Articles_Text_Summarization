{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Extractive Summarization - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Install Libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U datasets\n",
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edmun\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edmun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from typing import Dict, Any\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edmun\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for ccdv/pubmed-summarization contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ccdv/pubmed-summarization\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset from huggingface\n",
    "dataset = load_dataset('ccdv/pubmed-summarization', split=\"test\")\n",
    "\n",
    "# Take only 125 records from specified seed\n",
    "test_data = dataset.shuffle(seed=42).select(range(125))\n",
    "test_df = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Transformer Based Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def summarize(text, model, tokenizer, num_sentences=5):\n",
    "    # Lower the text and tokenize into sentences\n",
    "    sentences = nltk.sent_tokenize(text.lower())\n",
    "\n",
    "    # Tokenize each sentence and prepare for model input\n",
    "    tokenized_batches = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128, add_special_tokens=True)\n",
    "\n",
    "    # Process tokenized text through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{key: tokenized_batches[key].to(model.device) for key in tokenized_batches})\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Get embeddings for [CLS] token\n",
    "\n",
    "    # Calculate norms and sort by scores\n",
    "    scores = torch.norm(embeddings, dim=1)\n",
    "    sorted_indices = torch.argsort(scores, descending=True)\n",
    "\n",
    "    # Select top sentences based on sorted indices\n",
    "    best_sentences = [sentences[idx] for idx in sorted_indices[:num_sentences]]\n",
    "    return ' '.join(best_sentences)\n",
    "\n",
    "test_df['generated_summary'] = test_df['article'].apply(lambda x: summarize(x, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(test_df['generated_summary'])\n",
    "references = list(test_df['abstract'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edmun\\AppData\\Local\\Temp\\ipykernel_2280\\3162758740.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "c:\\Users\\edmun\\anaconda3\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.30750846788825115, recall=0.275241859700722, fmeasure=0.27130564440688104), mid=Score(precision=0.32912532624236135, recall=0.29860603549467823, fmeasure=0.2875972299071088), high=Score(precision=0.3516718735854453, recall=0.3213135107474994, fmeasure=0.30355767600590294)), 'rouge2': AggregateScore(low=Score(precision=0.07273991749532677, recall=0.06599572691519101, fmeasure=0.06481791224325473), mid=Score(precision=0.08660828906178913, recall=0.07746268327689801, fmeasure=0.07429792990260478), high=Score(precision=0.10424350162825315, recall=0.09227525471415907, fmeasure=0.08545589611269577)), 'rougeL': AggregateScore(low=Score(precision=0.16040256451945498, recall=0.14517032640371555, fmeasure=0.1421760137620293), mid=Score(precision=0.1752389767907766, recall=0.15716670142515324, fmeasure=0.15089707331228103), high=Score(precision=0.1903922134676424, recall=0.17094601441078375, fmeasure=0.15961185813742546)), 'rougeLsum': AggregateScore(low=Score(precision=0.2532952204776719, recall=0.22611742263843165, fmeasure=0.22221630280268678), mid=Score(precision=0.272468347541472, recall=0.24321411378112537, fmeasure=0.23618057581688123), high=Score(precision=0.29425022834606934, recall=0.2616146821794608, fmeasure=0.24892426696261108))}\n",
      "rouge1: Precision ranges from 30.75% to 35.17%, Recall ranges from 27.52% to 32.13%, F1 Score ranges from 27.13% to 30.36%.\n",
      "rouge2: Precision ranges from 7.27% to 10.42%, Recall ranges from 6.60% to 9.23%, F1 Score ranges from 6.48% to 8.55%.\n",
      "rougeL: Precision ranges from 16.04% to 19.04%, Recall ranges from 14.52% to 17.09%, F1 Score ranges from 14.22% to 15.96%.\n",
      "rougeLsum: Precision ranges from 25.33% to 29.43%, Recall ranges from 22.61% to 26.16%, F1 Score ranges from 22.22% to 24.89%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(rouge_scores)\n",
    "\n",
    "def simplify_rouge_scores(rouge_scores: Dict[str, Any]) -> str:\n",
    "    simplified_text = \"\"\n",
    "    for key, value in rouge_scores.items():\n",
    "        # Extract low, mid, and high scores for each ROUGE metric\n",
    "        low, mid, high = value.low, value.mid, value.high\n",
    "        simplified_text += f\"{key}: Precision ranges from {low.precision:.2%} to {high.precision:.2%}, \"\n",
    "        simplified_text += f\"Recall ranges from {low.recall:.2%} to {high.recall:.2%}, \"\n",
    "        simplified_text += f\"F1 Score ranges from {low.fmeasure:.2%} to {high.fmeasure:.2%}.\\n\"\n",
    "\n",
    "    return simplified_text\n",
    "\n",
    "print(simplify_rouge_scores(rouge_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.30750846788825115, recall=0.275241859700722, fmeasure=0.27130564440688104), mid=Score(precision=0.32912532624236135, recall=0.29860603549467823, fmeasure=0.2875972299071088), high=Score(precision=0.3516718735854453, recall=0.3213135107474994, fmeasure=0.30355767600590294)), 'rouge2': AggregateScore(low=Score(precision=0.07273991749532677, recall=0.06599572691519101, fmeasure=0.06481791224325473), mid=Score(precision=0.08660828906178913, recall=0.07746268327689801, fmeasure=0.07429792990260478), high=Score(precision=0.10424350162825315, recall=0.09227525471415907, fmeasure=0.08545589611269577)), 'rougeL': AggregateScore(low=Score(precision=0.16040256451945498, recall=0.14517032640371555, fmeasure=0.1421760137620293), mid=Score(precision=0.1752389767907766, recall=0.15716670142515324, fmeasure=0.15089707331228103), high=Score(precision=0.1903922134676424, recall=0.17094601441078375, fmeasure=0.15961185813742546)), 'rougeLsum': AggregateScore(low=Score(precision=0.2532952204776719, recall=0.22611742263843165, fmeasure=0.22221630280268678), mid=Score(precision=0.272468347541472, recall=0.24321411378112537, fmeasure=0.23618057581688123), high=Score(precision=0.29425022834606934, recall=0.2616146821794608, fmeasure=0.24892426696261108))}\n"
     ]
    }
   ],
   "source": [
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edmun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\edmun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average METEOR Score: 0.2125544343018366\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def evaluate_summaries_meteor(df, summary_col, reference_col):\n",
    "    # Tokenize summaries and references before passing to meteor_score\n",
    "    scores = [\n",
    "        meteor_score(\n",
    "            [nltk.word_tokenize(row[reference_col])], \n",
    "            nltk.word_tokenize(row[summary_col])\n",
    "        ) for _, row in df.iterrows()\n",
    "    ]\n",
    "    return sum(scores) / len(scores)  # Calculate the average METEOR score\n",
    "\n",
    "# Assuming 'test_df' has the columns 'generated_summary' and 'reference_summary'\n",
    "meteor_average_score = evaluate_summaries_meteor(test_df, 'generated_summary', 'abstract')\n",
    "print(\"Average METEOR Score:\", meteor_average_score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
